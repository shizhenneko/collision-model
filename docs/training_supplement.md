# VOA & ANN 训练详解与强化学习实施指南 (Supplementary Document)

本文档是对 `plan.md` 的深度补充，重点解析 **VOA (Policy Network)** 和 **ANN (Collision Model)** 的内部工作机制、训练细节，以及如何在**仅有离线数据**的情况下引入**强化学习 (RL)** 的思想来优化模型。

---

## 1. 核心网络功能与训练详解 (Network Deep Dive)

### 1.1 VOA Policy Network (策略网络)
这是机器人的“大脑”，负责驾驶。

*   **功能**: 建立从“感知状态”到“运动指令”的映射。
    *   **Input**: 经 **Gated Fusion 加权**的多模态融合特征向量 (State Embedding, 448维)。门控模块会根据环境自动抑制噪声模态（如暗光下的视觉）。
    *   **Output**: 动作向量 $\mathbf{a} = [v, \omega]$ (线速度, 角速度)。
*   **训练方法**: **行为克隆 (Behavior Cloning, BC)**。
    *   **本质**: 监督回归 (Supervised Regression)。
    *   **数据流**:
        1.  从数据集读取 $(State_t, Action_{expert\_t})$。
        2.  网络预测 $\hat{Action}_t = \pi(State_t)$。
        3.  计算损失 $Loss = ||\hat{Action}_t - Action_{expert\_t}||^2$ (MSE Loss)。
        4.  反向传播更新网络权重。
*   **物理意义**: “专家（你）在遇到这种情况时是怎么做的，我就怎么做。”

### 1.2 ANN Collision Model (碰撞预测网络)
这是机器人的“直觉”，负责评估风险。

*   **功能**: 建立从“感知状态”到“碰撞概率”的映射。
    *   **Input**: 经 **Gated Fusion 加权**的多模态融合特征向量 (State Embedding, 448维)。
    *   **Output**: 标量概率 $P_{crash} \in [0, 1]$。
*   **训练方法**: **二元分类 (Binary Classification)**。
    *   **本质**: 监督分类。
    *   **数据流**:
        1.  从数据集读取 $State_t$。
        2.  **自动标签生成 (关键)**: 检查 $t$ 时刻后 $T$ 秒内的 Lidar 数据。若存在 $Distance < 0.2m$，则 Label $y=1$ (危险)，否则 $y=0$ (安全)。
        3.  网络预测 $\hat{y}_t = Sigmoid(Network(State_t))$。
        4.  计算损失 $Loss = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$ (BCE Loss)。
*   **物理意义**: “当前这种局面，根据历史经验，接下来 $T$ 秒内撞车的概率有多大？”

### 1.3 VOA Multi-Time Scale Policy Network (多时间尺度策略网络)
这是机器人的“前瞻规划器”，负责预测未来多个时间尺度的动作序列。

*   **功能**: 建立从“当前感知状态”到“未来多时间尺度动作序列”的映射。
    *   **Input**: 经 **Gated Fusion 加权**的多模态融合特征向量 (State Embedding, 448维)。
    *   **Output**: 多时间尺度动作向量 $[v_{t+0.1}, \omega_{t+0.1}, v_{t+0.5}, \omega_{t+0.5}, v_{t+1.0}, \omega_{t+1.0}]$。
*   **训练方法**: **多输出回归学习 (Multi-Output Regression)**。
    *   **本质**: 监督回归，同时预测多个时间点的动作。
    *   **数据流**:
        1.  从数据集读取 $State_t$。
        2.  **多时间尺度标签生成**: 提取专家遥控指令的未来序列：
            *   $a_{t+0.1}$: 0.1秒后的动作（短期）
            *   $a_{t+0.5}$: 0.5秒后的动作（中期）  
            *   $a_{t+1.0}$: 1.0秒后的动作（长期）
        3.  网络预测 $\hat{A}_t = MultiScaleNetwork(State_t)$。
        4.  计算多尺度损失 $Loss = \sum_{i=1}^{3} \alpha_i \cdot ||\hat{a}_i - a_{expert,i}||^2$。
*   **物理意义**: “面对当前情况，专家在短期、中期、长期分别会采取什么动作？这为ANN强制制动后的动作续接提供了候选方案。”

*   **推理应用**:
    *   **正常状态**: 使用短期预测 $[v_{t+0.1}, \omega_{t+0.1}]$ 作为当前控制指令
    *   **ANN触发**: 执行紧急制动，同时记录制动持续时间 $\Delta t_{brake}$
    *   **动作续接**: 根据制动时间选择合适的时间尺度预测：
        *   $\Delta t_{brake} < 0.3s$: 使用中期预测恢复
        *   $\Delta t_{brake} \geq 0.3s$: 使用长期预测重新规划
    *   **平滑过渡**: 采用线性插值确保动作连续性，避免突变

---

## 2. 如何使用强化学习 (RL) 结合您的数据

您目前拥有的是**静态的离线数据集 (Offline Data)**，而不是一个可以交互的仿真器。标准的强化学习 (如 PPO, DQN) 需要模型与环境不断交互（试错），因此无法直接使用。

但是，我们可以借鉴 **Offline RL (离线强化学习)** 的思想，利用 **ANN 模型作为“奖励函数 (Reward Model)”** 来优化 **VOA 策略**。

### 方案 A: 安全过滤行为克隆 (Safety-Filtered BC) —— *推荐入门*
这是最简单且有效的“类 RL”方法。

*   **核心思想**: 并不是专家（你）的所有操作都是完美的。有时候你也可能差点撞车，或者操作不当。我们要让 VOA **只学习专家“安全”的操作，抛弃“危险”的操作**。
*   **实施步骤**:
    1.  **Step 1**: 先训练 **ANN Collision Model**，直到它能准确预测风险。
    2.  **Step 2**: 使用训练好的 ANN 对整个数据集进行“清洗”。
        *   将数据输入 ANN，得到风险分 $P_{crash}$。
        *   **Filter**: 如果某帧数据的 $P_{crash} > 0.3$ (阈值)，说明当时情况危急或专家操作导致了风险，**将这帧数据从 VOA 的训练集中剔除**。
    3.  **Step 3**: 使用清洗后的“纯净安全数据”训练 **VOA Policy**。
*   **效果**: 机器学到的策略会比原始的专家策略更保守、更安全。

### 方案 B: 奖励加权回归 (Reward-Weighted Regression, RWR) —— *进阶*
这是 Offline RL 的一种简化形式。

*   **核心思想**: 不直接剔除数据，而是给“好”的数据更高的权重，给“坏”的数据极低的权重。
*   **定义奖励 (Reward)**: 我们定义奖励 $R = 1 - P_{crash}$ (越安全，奖励越高)。
*   **训练公式修改**:
    *   原始 Loss: $Loss = ||\hat{a} - a_{expert}||^2$
    *   加权 Loss: $Loss = e^{(R / T)} \cdot ||\hat{a} - a_{expert}||^2$
    *   *解释*: 如果 ANN 认为某时刻非常安全 ($R \approx 1$)，Loss 权重变大，网络重点学习该动作；如果 ANN 认为危险 ($R \approx 0$)，Loss 权重变小，网络忽略该动作。
*   **多时间尺度扩展**: 对于多时间尺度VOA预测，每个时间尺度的动作都应用相同的奖励权重：
    *   $Loss_{multi} = e^{(R / T)} \cdot \sum_{i=1}^{3} \alpha_i \cdot ||\hat{a}_i - a_{expert,i}||^2$
    *   其中 $\alpha_i$ 为各时间尺度的权重系数，反映不同预测时域的重要性。
*   **实施**: 在 PyTorch 的 Loss 函数中乘上这个权重系数即可。

### 方案 C: 真正的 Offline RL (如 CQL) —— *高难度*
*   **适用场景**: 如果方案 A/B 效果不佳，且您有极强的深度学习背景。
*   **算法**: Conservative Q-Learning (CQL) 或 IQL。
*   **原理**: 同时学习 Q 函数（评估动作价值）和 Policy（策略），并对 Q 值进行惩罚以防止对未见过的动作高估。
*   **建议**: 对于本项目，**不推荐**首选此方案，因为调参极其困难且容易不收敛。

---

## 3. 训练流程总结 (Summary Roadmap)

结合上述分析，您的最佳训练路径是：

1.  **采集数据**: 尽可能多地跑，允许偶尔的轻微碰撞（为了给 ANN 提供负样本）。
2.  **训练 ANN**: 让机器学会识别“什么是危险”。
    *   *Check*: 在验证集上查看 ANN 的准确率。
3.  **数据清洗 (RL Step)**: 利用 ANN 给数据打分，筛选出高质量数据。
4.  **训练 VOA Multi-Time Scale**: 在高质量数据上学习多时间尺度动作预测。
    *   *关键*: 生成多时间尺度标签，训练网络预测未来多个时间点的动作序列。
5.  **训练 VOA Single-Time Scale**: 在高质量数据上进行标准行为克隆。
6.  **部署验证**: 部署到机器人，测试ANN强制停止后的动作续接能力。

这种方法即利用了监督学习的稳定性，又融入了强化学习“趋利避害”的核心思想，同时解决了ANN干预后的动作续接问题。
