# VOA & ANN 训练详解与强化学习实施指南 (Supplementary Document)

本文档是对 `plan.md` 的深度补充，重点解析 **VOA (Policy Network)** 和 **ANN (Collision Model)** 的内部工作机制、训练细节，以及如何在**仅有离线数据**的情况下引入**强化学习 (RL)** 的思想来优化模型。

---

## 1. 核心网络功能与训练详解 (Network Deep Dive)

### 1.1 VOA Policy Network (策略网络)
这是机器人的“大脑”，负责驾驶。

*   **功能**: 建立从“感知状态”到“运动指令”的映射。
    *   **Input**: 经 **Gated Fusion 加权**的多模态融合特征向量 (State Embedding, 448维)。门控模块会根据环境自动抑制噪声模态（如暗光下的视觉）。
    *   **Output**: 动作向量 $\mathbf{a} = [v, \omega]$ (线速度, 角速度)。
*   **训练方法**: **行为克隆 (Behavior Cloning, BC)**。
    *   **本质**: 监督回归 (Supervised Regression)。
    *   **数据流**:
        1.  从数据集读取 $(State_t, Action_{expert\_t})$。
        2.  网络预测 $\hat{Action}_t = \pi(State_t)$。
        3.  计算损失 $Loss = ||\hat{Action}_t - Action_{expert\_t}||^2$ (MSE Loss)。
        4.  反向传播更新网络权重。
*   **物理意义**: “专家（你）在遇到这种情况时是怎么做的，我就怎么做。”

### 1.2 ANN Collision Model (碰撞预测网络)
这是机器人的“直觉”，负责评估风险。

*   **功能**: 建立从“感知状态”到“碰撞概率”的映射。
    *   **Input**: 经 **Gated Fusion 加权**的多模态融合特征向量 (State Embedding, 448维)。
    *   **Output**: 标量概率 $P_{crash} \in [0, 1]$。
*   **训练方法**: **二元分类 (Binary Classification)**。
    *   **本质**: 监督分类。
    *   **数据流**:
        1.  从数据集读取 $State_t$。
        2.  **自动标签生成 (关键)**: 检查 $t$ 时刻后 $T$ 秒内的 Lidar 数据。若存在 $Distance < 0.2m$，则 Label $y=1$ (危险)，否则 $y=0$ (安全)。
        3.  网络预测 $\hat{y}_t = Sigmoid(Network(State_t))$。
        4.  计算损失 $Loss = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$ (BCE Loss)。
*   **物理意义**: “当前这种局面，根据历史经验，接下来 $T$ 秒内撞车的概率有多大？”

---

## 2. 如何使用强化学习 (RL) 结合您的数据

您目前拥有的是**静态的离线数据集 (Offline Data)**，而不是一个可以交互的仿真器。标准的强化学习 (如 PPO, DQN) 需要模型与环境不断交互（试错），因此无法直接使用。

但是，我们可以借鉴 **Offline RL (离线强化学习)** 的思想，利用 **ANN 模型作为“奖励函数 (Reward Model)”** 来优化 **VOA 策略**。

### 方案 A: 安全过滤行为克隆 (Safety-Filtered BC) —— *推荐入门*
这是最简单且有效的“类 RL”方法。

*   **核心思想**: 并不是专家（你）的所有操作都是完美的。有时候你也可能差点撞车，或者操作不当。我们要让 VOA **只学习专家“安全”的操作，抛弃“危险”的操作**。
*   **实施步骤**:
    1.  **Step 1**: 先训练 **ANN Collision Model**，直到它能准确预测风险。
    2.  **Step 2**: 使用训练好的 ANN 对整个数据集进行“清洗”。
        *   将数据输入 ANN，得到风险分 $P_{crash}$。
        *   **Filter**: 如果某帧数据的 $P_{crash} > 0.3$ (阈值)，说明当时情况危急或专家操作导致了风险，**将这帧数据从 VOA 的训练集中剔除**。
    3.  **Step 3**: 使用清洗后的“纯净安全数据”训练 **VOA Policy**。
*   **效果**: 机器学到的策略会比原始的专家策略更保守、更安全。

### 方案 B: 奖励加权回归 (Reward-Weighted Regression, RWR) —— *进阶*
这是 Offline RL 的一种简化形式。

*   **核心思想**: 不直接剔除数据，而是给“好”的数据更高的权重，给“坏”的数据极低的权重。
*   **定义奖励 (Reward)**: 我们定义奖励 $R = 1 - P_{crash}$ (越安全，奖励越高)。
*   **训练公式修改**:
    *   原始 Loss: $Loss = ||\hat{a} - a_{expert}||^2$
    *   加权 Loss: $Loss = e^{(R / T)} \cdot ||\hat{a} - a_{expert}||^2$
    *   *解释*: 如果 ANN 认为某时刻非常安全 ($R \approx 1$)，Loss 权重变大，网络重点学习该动作；如果 ANN 认为危险 ($R \approx 0$)，Loss 权重变小，网络忽略该动作。
*   **实施**: 在 PyTorch 的 Loss 函数中乘上这个权重系数即可。

### 方案 C: 真正的 Offline RL (如 CQL) —— *高难度*
*   **适用场景**: 如果方案 A/B 效果不佳，且您有极强的深度学习背景。
*   **算法**: Conservative Q-Learning (CQL) 或 IQL。
*   **原理**: 同时学习 Q 函数（评估动作价值）和 Policy（策略），并对 Q 值进行惩罚以防止对未见过的动作高估。
*   **建议**: 对于本项目，**不推荐**首选此方案，因为调参极其困难且容易不收敛。

---

## 3. 训练流程总结 (Summary Roadmap)

结合上述分析，您的最佳训练路径是：

1.  **采集数据**: 尽可能多地跑，允许偶尔的轻微碰撞（为了给 ANN 提供负样本）。
2.  **训练 ANN**: 让机器学会识别“什么是危险”。
    *   *Check*: 在验证集上查看 ANN 的准确率。
3.  **数据清洗 (RL Step)**: 利用 ANN 给数据打分，筛选出高质量数据。
4.  **训练 VOA**: 在高质量数据上进行行为克隆。
5.  **部署验证**: 部署到机器人，观察是否比单纯模仿更安全。

这种方法即利用了监督学习的稳定性，又融入了强化学习“趋利避害”的核心思想。
